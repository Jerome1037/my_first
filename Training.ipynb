{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "325352cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8163179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载MNIST数据集\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "#数据集处理\n",
    "x_train = x_train[0:10000]/255.0\n",
    "x_test = x_test[0:6000]/255.0\n",
    "x_train = x_train.reshape(10000,1,28,28)\n",
    "x_test = x_test.reshape(6000,1,28,28)\n",
    "#转化为onehot标签\n",
    "onehot = np.identity(10)\n",
    "y_train = onehot[y_train[:10000]]\n",
    "y_test = onehot[y_test[:6000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0214b0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(input_data,kernel_size, stride=1, pad=0):\n",
    "    #im2col函数将输入的图片数据转化为col\n",
    "    N, C, H, W = input_data.shape\n",
    "    filter_h = kernel_size\n",
    "    filter_w = kernel_size\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1 #将卷积核的大小赋值 \n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad,pad), (pad,pad)], 'constant')  #填充操作\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w)) #初始化\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col\n",
    "\n",
    "\n",
    "def col2im(col, input_shape, kernel_size, stride=1, pad=0):\n",
    "    #col2im是im2col的逆操作\n",
    "    N, C, H, W = input_shape\n",
    "    filter_h = kernel_size\n",
    "    filter_w = kernel_size\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4168394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Conv2D:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=1):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.t = 0\n",
    "        self.m = 0\n",
    "        self.v = 0\n",
    "        \n",
    "        # 初始化卷积核权重和偏置\n",
    "        self.weights = np.random.randn(out_channels, in_channels, kernel_size, kernel_size)*0.01\n",
    "        self.bias = np.zeros((out_channels, 1))\n",
    "        \n",
    "        # 初始化用于反向传播的中间变量\n",
    "        self.x = None\n",
    "        self.col_x = None\n",
    "        self.col_weight = None\n",
    "        self.dweight = None\n",
    "        self.dbias = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 输入x的维度为 (batch_size, in_channels, height, width)\n",
    "        batch_size, in_channels, in_height, in_width = x.shape\n",
    "        \n",
    "        # 计算输出特征图的大小\n",
    "        out_height = int((in_height + 2 * self.padding - self.kernel_size) / self.stride + 1)\n",
    "        out_width = int((in_width + 2 * self.padding - self.kernel_size) / self.stride + 1)\n",
    "        \n",
    "        # 使用im2col函数将输入x变换为二维矩阵col_x\n",
    "        self.col_x = im2col(x, self.kernel_size, self.stride, self.padding)\n",
    "        \n",
    "        # 使用reshape函数将卷积核权重变换为二维矩阵col_weight\n",
    "        self.col_weight = self.weights.reshape(self.out_channels, -1).T\n",
    "        \n",
    "        # 计算卷积的结果\n",
    "        out = np.dot(self.col_x, self.col_weight) + self.bias.T\n",
    "        \n",
    "        # 将结果重塑为输出特征图的形状\n",
    "        out = out.reshape(batch_size, out_height, out_width, self.out_channels)\n",
    "        out = out.transpose(0, 3, 1, 2)\n",
    "        \n",
    "        # 保存用于反向传播的中间变量\n",
    "        self.x = x\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout,learning_rate):\n",
    "        # 输入dout的维度为 (batch_size, out_channels, out_height, out_width)\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        # 使用reshape函数将dout变换为二维矩阵\n",
    "        dout = dout.reshape(-1, self.out_channels)\n",
    "        \n",
    "        # 计算卷积核权重的梯度\n",
    "        self.dweight = np.dot(self.col_x.T, dout)\n",
    "        self.dweight = self.dweight.transpose(1, 0).reshape(self.out_channels, self.in_channels, self.kernel_size, self.kernel_size)\n",
    "        \n",
    "        # 计算偏置的梯度\n",
    "        self.dbias = np.sum(dout, axis=0, keepdims=True)\n",
    "        \n",
    "        # 计算输入x的梯度\n",
    "        dcol_x = np.dot(dout, self.col_weight.T)\n",
    "        dx = col2im(dcol_x, self.x.shape, self.kernel_size, self.stride, self.padding)\n",
    "        \n",
    "        #Adam算法\n",
    "        eps = 1e-8\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.999\n",
    "        self.t += 1\n",
    "        self.m = beta1 * self.m + (1 - beta1) * self.dweight\n",
    "        self.v = beta2 * self.v + (1 - beta2) * self.dweight ** 2\n",
    "        m_hat = self.m / (1 - beta1 ** self.t)\n",
    "        v_hat = self.v / (1 - beta2 ** self.t)\n",
    "        self.weights -= learning_rate * m_hat / (np.sqrt(v_hat) + eps)\n",
    "        self.bias -= learning_rate * self.dbias.T      \n",
    "        return dx\n",
    "    \n",
    "    def save(self, file_name): #save和load用以保存训练完成的权重和偏置，此后相同，不再赘述\n",
    "        #保存为.npy格式\n",
    "        np.savez(file_name, weights=self.weights, bias=self.bias)\n",
    "\n",
    "    def load(self, file_name):\n",
    "        #将.npy读入\n",
    "        data = np.load(file_name)\n",
    "        self.weights = data['weights']\n",
    "        self.bias = data['bias']\n",
    "        \n",
    "class ReLU:\n",
    "    #ReLU激活函数的实现\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = x <= 0 #mask用于保存x<=0的位置，方便方向传播运算\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0 #小于0的位置赋值为0\n",
    "        #print(np.shape(out))\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def backward(self, dout): #ReLU的反向传播较为简单，x<0处为0，x>0处直接等于dout\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx\n",
    "\n",
    "class MaxPool2D: #最大池化操作\n",
    "    def __init__(self, pool_size=(2,2), stride=(2,2)):\n",
    "        #初始化操作\n",
    "        self.pool_h, self.pool_w = pool_size\n",
    "        self.stride_h, self.stride_w = stride\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        #前向传播\n",
    "        N, C, H, W = x.shape\n",
    "        #计算输出的形状\n",
    "        out_h = int((H - self.pool_h) / self.stride_h + 1)\n",
    "        out_w = int((W - self.pool_w) / self.stride_w + 1)\n",
    "        #初始化\n",
    "        out = np.zeros((N, C, out_h, out_w))\n",
    "        \n",
    "        #滑窗最大池化操作\n",
    "        for h in range(out_h):\n",
    "            for w in range(out_w):\n",
    "                #对当前输出格，先将对应位置的最大池化window选出\n",
    "                h_start, w_start = h*self.stride_h, w*self.stride_w\n",
    "                h_end, w_end = h_start+self.pool_h, w_start+self.pool_w\n",
    "                window = x[:, :, h_start:h_end, w_start:w_end]\n",
    "                #再将对应的最大值赋值给当前输出\n",
    "                out[:, :, h, w] = np.max(window, axis=(2,3))\n",
    "        #print(out[0,0])\n",
    "        #记录数据便于反向传播\n",
    "        self.cache = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        #最大池化的反向传播\n",
    "        x = self.cache\n",
    "        N, C, H, W = x.shape\n",
    "        _, _, out_h, out_w = dout.shape\n",
    "        \n",
    "        #初始化为0\n",
    "        dx = np.zeros_like(x)\n",
    "        \n",
    "        #最大值处有梯度下降反向传播，其余位置梯度下降为0\n",
    "        for h in range(out_h):\n",
    "            for w in range(out_w):\n",
    "                #和前向相同，先提取出当前的window\n",
    "                h_start, w_start = h*self.stride_h, w*self.stride_w\n",
    "                h_end, w_end = h_start+self.pool_h, w_start+self.pool_w\n",
    "                window = x[:, :, h_start:h_end, w_start:w_end]\n",
    "                #寻找最大值\n",
    "                max_value = np.max(window, axis=(2,3))\n",
    "                max_mask = (window == (max_value)[:,:,None,None])\n",
    "                #将最大值处的梯度反向传播\n",
    "                dx[:, :, h_start:h_end, w_start:w_end] += max_mask * (dout[:, :, h, w])[:,:,None,None]\n",
    "\n",
    "        return dx    \n",
    "        \n",
    "class Softmax():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, input):  \n",
    " \n",
    "        # softmax 计算输出结果\n",
    "        input_exp = np.exp(input)#生成(n,class)大小的e^(每个类别的概率)\n",
    "        sum_denominator = np.sum(input_exp,axis=1,keepdims=True)#取每行的和,变成（n,1)\n",
    "        self.prob = input_exp / sum_denominator#得到softmax最终公式（即属于每一类的概率）,分母（n，1）拓展成（n，c）\n",
    "        #print(self.prob)\n",
    "        return self.prob\n",
    "    \n",
    "    def get_loss(self,y_onehot):   \n",
    "        #计算损失\n",
    "        self.batch_size = self.prob.shape[0]#n\n",
    "        #构建one-hot标签\n",
    "        self.y_onehot = y_onehot\n",
    "        #self.label_onehot = np.zeros_like(self.prob)\n",
    "        #self.label_onehot[np.arange(self.batch_size), label] = 1.0 #第几个样本最终属于哪一类(概率为1，其他为0)\n",
    "        #计算交叉熵，此处的1e-9是为了避免出现对0运算的情况\n",
    "        loss = -np.sum(np.log(self.prob+1e-9) * self.y_onehot) / self.batch_size\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):  \n",
    "        #反向传播的计算\n",
    "        #计算本层损失（这边要的是一个梯度矩阵）\n",
    "        bottom_diff = (self.prob - self.y_onehot)/self.batch_size        \n",
    "        return bottom_diff\n",
    "\n",
    "\n",
    "class Accuracy:\n",
    "    def __init__(self):\n",
    "        self.acc = None\n",
    "    \n",
    "    def acc(self, y_pred, y_true):\n",
    "        # 计算准确率\n",
    "        pred_label = np.argmax(y_pred, axis=1)\n",
    "        true_label = np.argmax(y_true, axis=1)\n",
    "        #真实标签和预测标签对比\n",
    "        self.acc = np.sum(pred_label == true_label) / y_pred.shape[0]\n",
    "        \n",
    "        return self.acc\n",
    "    \n",
    "class FullConect: #全连接\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * np.sqrt(1.0 / input_dim)\n",
    "        self.biases = np.zeros(output_dim)\n",
    "        self.x_shape = None\n",
    "        self.d_weights= None\n",
    "        self.d_biases = None\n",
    "        self.x = None\n",
    "        self.t = 0\n",
    "        self.m = 0\n",
    "        self.v = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #全连接前向传播\n",
    "        self.x_shape = x.shape\n",
    "        #将(N,C,H,W)的后三个维度展平为(N，C*H*W)\n",
    "        self.x =  x.reshape(x.shape[0],-1) \n",
    "        #print(np.shape(self.x))\n",
    "        \n",
    "        #前向传播是简单的矩阵乘法运算\n",
    "        return np.dot(self.x, self.weights) + self.biases\n",
    "    \n",
    "\n",
    "    def backward(self, dout,lr):\n",
    "        learning_rate = lr\n",
    "        self.d_weights = np.dot(self.x.T, dout)\n",
    "        self.d_biases = np.sum(dout, axis=0)\n",
    "        dx = np.dot(dout, self.weights.T)\n",
    "        dx = dx.reshape(self.x_shape)\n",
    "        \n",
    "        #更新权重和偏置\n",
    "        # Adam算法\n",
    "        eps = 1e-8\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.999\n",
    "        self.t += 1\n",
    "        self.m = beta1 * self.m + (1 - beta1) * self.d_weights\n",
    "        self.v = beta2 * self.v + (1 - beta2) * self.d_weights ** 2\n",
    "        m_hat = self.m / (1 - beta1 ** self.t)\n",
    "        v_hat = self.v / (1 - beta2 ** self.t)\n",
    "        self.weights -= learning_rate * m_hat / (np.sqrt(v_hat) + eps)\n",
    "        self.biases -= learning_rate * self.d_biases     \n",
    "        return dx\n",
    "    \n",
    "    def save(self, file_name):\n",
    "        np.savez(file_name, weights=self.weights, bias=self.biases)\n",
    "\n",
    "    def load(self, file_name):\n",
    "        data = np.load(file_name)\n",
    "        self.weights = data['weights']\n",
    "        self.biases = data['bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab54bdba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18f81807",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet:\n",
    "    def __init__(self):\n",
    "        # 定义卷积层的超参数       \n",
    "        self.conv_layer1 = Conv2D(1,16,3)\n",
    "        self.relu_layer1 = ReLU()\n",
    "        self.pool_layer1 = MaxPool2D()\n",
    "        self.conv_layer2 = Conv2D(16, 64, 3)\n",
    "        self.relu_layer2 = ReLU()\n",
    "        self.pool_layer2 = MaxPool2D()\n",
    "        self.fc_layer1 = FullConect(64*7*7,4096)\n",
    "        self.fc_layer2 = FullConect(4096,1024)\n",
    "        self.fc_layer3 = FullConect(1024,10)\n",
    "        self.softmax_layer = Softmax()\n",
    "        self.learningrate = 0.0001\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out_conv1 = self.conv_layer1.forward(x)\n",
    "        out_relu1 = self.relu_layer1.forward(out_conv1)\n",
    "        out_pool1 = self.pool_layer1.forward(out_relu1)\n",
    "        out_conv2 = self.conv_layer2.forward(out_pool1)\n",
    "        out_relu2 = self.relu_layer2.forward(out_conv2)\n",
    "        out_pool2 = self.pool_layer2.forward(out_relu2)\n",
    "        out_fc1 = self.fc_layer1.forward(out_pool2)\n",
    "        out_fc2 = self.fc_layer2.forward(out_fc1)\n",
    "        out_fc3 = self.fc_layer3.forward(out_fc2)\n",
    "        out_softmax = self.softmax_layer.forward(out_fc3)\n",
    "        y_pred = self.to_onehot(out_softmax)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def backward(self,lr):\n",
    "        d_softmax = self.softmax_layer.backward()\n",
    "        d_fc3 = self.fc_layer3.backward(d_softmax,0.001)\n",
    "        d_fc2 = self.fc_layer2.backward(d_fc3,0.0009)\n",
    "        d_fc1 = self.fc_layer1.backward(d_fc2,0.0005)\n",
    "        d_pool2 = self.pool_layer2.backward(d_fc1)\n",
    "        d_relu2 = self.relu_layer2.backward(d_pool2)\n",
    "        d_conv2 = self.conv_layer2.backward(d_relu2,0.0005)\n",
    "        d_pool1 = self.pool_layer1.backward(d_conv2)\n",
    "        d_relu1 = self.relu_layer1.backward(d_pool1)\n",
    "        d_conv1 = self.conv_layer1.backward(d_relu1,0.0005)\n",
    "\n",
    "    def to_onehot(self,probs): #softmax转化为onehot标签\n",
    "        num_samples = probs.shape[0]\n",
    "        num_classes = probs.shape[1]\n",
    "        onehot = np.zeros((num_samples, num_classes))\n",
    "        onehot[np.arange(num_samples), np.argmax(probs, axis=1)] = 1\n",
    "        return onehot\n",
    "    \n",
    "    def predict(self,x):         \n",
    "        y_pred = self.forward(x)\n",
    "        y_onehot = self.to_onehot(y_pred)\n",
    "        return y_onehot,y_pred\n",
    "    \n",
    "    def accuracy(self,y_pred, y_true):\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "        accuracy = np.mean(y_pred == y_true)\n",
    "        return accuracy\n",
    "\n",
    "    def save(self, file_name):\n",
    "        #保存参数权重的函数\n",
    "        save_dir = os.path.expanduser('~/model_save')\n",
    "        #当前路径下\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        #对每一层的参数进行保存\n",
    "        filepath = os.path.join(save_dir, file_name)\n",
    "        data = {\n",
    "            'conv_layer1': self.conv_layer1.save(filepath + \"_conv_layer1\"),\n",
    "            'conv_layer2': self.conv_layer2.save(filepath + \"_conv_layer2\"),\n",
    "            'fc_layer1': self.fc_layer1.save(filepath + \"_fc_layer1\"),\n",
    "            'fc_layer2': self.fc_layer2.save(filepath + \"_fc_layer2\"),\n",
    "            'fc_layer3': self.fc_layer3.save(filepath + \"_fc_layer3\")\n",
    "        }\n",
    "        with open(filepath + \".npz\", 'wb') as f:\n",
    "            np.savez(f, **data)\n",
    "    \n",
    "    def load(self, file_name):\n",
    "        #加载权重的函数\n",
    "        save_dir = os.path.expanduser('~/model_save')\n",
    "        filepath = os.path.join(save_dir, file_name)\n",
    "        with np.load(filepath + \".npz\") as data:\n",
    "            self.conv_layer1.load(filepath + \"_conv_layer1.npz\")\n",
    "            self.conv_layer2.load(filepath + \"_conv_layer2.npz\")\n",
    "            self.fc_layer1.load(filepath + \"_fc_layer1.npz\")\n",
    "            self.fc_layer2.load(filepath + \"_fc_layer2.npz\")\n",
    "            self.fc_layer3.load(filepath + \"_fc_layer3.npz\")\n",
    "            \n",
    "    def train(self,X,Y,test_X,test_Y):\n",
    "        lr = self.learningrate\n",
    "        epochs = 50\n",
    "        batch_size = 512\n",
    "        num_samples = X.shape[0]\n",
    "        num_batches = num_samples // batch_size\n",
    "        \n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        for i in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            # Shuffle the data for each epoch\n",
    "            permutation = np.random.permutation(num_samples)\n",
    "            X = X[permutation]\n",
    "            Y = Y[permutation]\n",
    "            \n",
    "            for j in range(num_batches):\n",
    "                # 将当前batch的数据选出\n",
    "                start = j * batch_size\n",
    "                end = start + batch_size\n",
    "                X_batch = X[start:end]\n",
    "                Y_batch = Y[start:end]\n",
    "                \n",
    "                #前向传播\n",
    "                y_pred = self.forward(X_batch)\n",
    "                \n",
    "                #计算loss\n",
    "                loss = self.softmax_layer.get_loss(Y_batch)\n",
    "                y_pred = self.to_onehot(y_pred)\n",
    "         \n",
    "                epoch_loss += loss\n",
    "            \n",
    "                #反向传播\n",
    "                self.backward(lr)\n",
    "\n",
    "                    \n",
    "            if i % 1 == 0:\n",
    "                # 预测测试集上的结果\n",
    "                test_preds,test_probs = self.predict(test_X)\n",
    "                # 计算测试集上的准确率\n",
    "                test_acc = self.accuracy(test_preds,test_Y)\n",
    "                print(\"Epoch {}/{}: Train Loss = {:.4f}, Test Accuracy = {:.4f}\".format(i+1, epochs, epoch_loss, test_acc))\n",
    "                train_loss.append(epoch_loss)\n",
    "                train_acc.append(test_acc)\n",
    "        return train_loss,train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdc98b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss = 27.9501, Test Accuracy = 0.7772\n",
      "Epoch 2/50: Train Loss = 8.6342, Test Accuracy = 0.8928\n",
      "Epoch 3/50: Train Loss = 4.9145, Test Accuracy = 0.9240\n",
      "Epoch 4/50: Train Loss = 3.2389, Test Accuracy = 0.9552\n",
      "Epoch 5/50: Train Loss = 2.4038, Test Accuracy = 0.9588\n",
      "Epoch 6/50: Train Loss = 1.8972, Test Accuracy = 0.9623\n",
      "Epoch 7/50: Train Loss = 1.5602, Test Accuracy = 0.9658\n",
      "Epoch 8/50: Train Loss = 1.2336, Test Accuracy = 0.9612\n",
      "Epoch 9/50: Train Loss = 1.1211, Test Accuracy = 0.9697\n",
      "Epoch 10/50: Train Loss = 0.9294, Test Accuracy = 0.9722\n",
      "Epoch 11/50: Train Loss = 0.7257, Test Accuracy = 0.9740\n",
      "Epoch 12/50: Train Loss = 0.6041, Test Accuracy = 0.9740\n",
      "Epoch 13/50: Train Loss = 0.5219, Test Accuracy = 0.9688\n",
      "Epoch 14/50: Train Loss = 0.5448, Test Accuracy = 0.9710\n",
      "Epoch 15/50: Train Loss = 0.4187, Test Accuracy = 0.9715\n",
      "Epoch 16/50: Train Loss = 0.6039, Test Accuracy = 0.9710\n",
      "Epoch 17/50: Train Loss = 0.4940, Test Accuracy = 0.9650\n",
      "Epoch 18/50: Train Loss = 0.3267, Test Accuracy = 0.9753\n",
      "Epoch 19/50: Train Loss = 0.2382, Test Accuracy = 0.9758\n",
      "Epoch 20/50: Train Loss = 0.1737, Test Accuracy = 0.9765\n",
      "Epoch 21/50: Train Loss = 0.1848, Test Accuracy = 0.9760\n",
      "Epoch 22/50: Train Loss = 0.1336, Test Accuracy = 0.9752\n",
      "Epoch 23/50: Train Loss = 0.0994, Test Accuracy = 0.9728\n",
      "Epoch 24/50: Train Loss = 0.1112, Test Accuracy = 0.9758\n",
      "Epoch 25/50: Train Loss = 0.0866, Test Accuracy = 0.9767\n",
      "Epoch 26/50: Train Loss = 0.0464, Test Accuracy = 0.9770\n",
      "Epoch 27/50: Train Loss = 0.0310, Test Accuracy = 0.9777\n",
      "Epoch 28/50: Train Loss = 0.0360, Test Accuracy = 0.9772\n",
      "Epoch 29/50: Train Loss = 0.0240, Test Accuracy = 0.9778\n",
      "Epoch 30/50: Train Loss = 0.0232, Test Accuracy = 0.9740\n",
      "Epoch 31/50: Train Loss = 0.0219, Test Accuracy = 0.9742\n",
      "Epoch 32/50: Train Loss = 0.0186, Test Accuracy = 0.9775\n",
      "Epoch 33/50: Train Loss = 0.0174, Test Accuracy = 0.9777\n",
      "Epoch 34/50: Train Loss = 0.0128, Test Accuracy = 0.9773\n",
      "Epoch 35/50: Train Loss = 0.0097, Test Accuracy = 0.9767\n",
      "Epoch 36/50: Train Loss = 0.0065, Test Accuracy = 0.9765\n",
      "Epoch 37/50: Train Loss = 0.0050, Test Accuracy = 0.9767\n",
      "Epoch 38/50: Train Loss = 0.0041, Test Accuracy = 0.9778\n",
      "Epoch 39/50: Train Loss = 0.0037, Test Accuracy = 0.9783\n",
      "Epoch 40/50: Train Loss = 0.0035, Test Accuracy = 0.9777\n",
      "Epoch 41/50: Train Loss = 0.0032, Test Accuracy = 0.9782\n",
      "Epoch 42/50: Train Loss = 0.0031, Test Accuracy = 0.9780\n",
      "Epoch 43/50: Train Loss = 0.0028, Test Accuracy = 0.9785\n",
      "Epoch 44/50: Train Loss = 0.0027, Test Accuracy = 0.9782\n",
      "Epoch 45/50: Train Loss = 0.0026, Test Accuracy = 0.9778\n",
      "Epoch 46/50: Train Loss = 0.0024, Test Accuracy = 0.9780\n",
      "Epoch 47/50: Train Loss = 0.0023, Test Accuracy = 0.9783\n",
      "Epoch 48/50: Train Loss = 0.0023, Test Accuracy = 0.9780\n",
      "Epoch 49/50: Train Loss = 0.0021, Test Accuracy = 0.9777\n",
      "Epoch 50/50: Train Loss = 0.0020, Test Accuracy = 0.9782\n"
     ]
    }
   ],
   "source": [
    "net = ConvNet()\n",
    "rain_loss,train_acc = net.train(x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "59c08453",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '01'\n",
    "net.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "715f0e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a0ba457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ce82b72550>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.imshow(x_train[9500][0],cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b81cc7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3329a276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11d491e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "无",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
